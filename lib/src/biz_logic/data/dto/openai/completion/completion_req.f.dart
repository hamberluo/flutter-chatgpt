// ignore_for_file: non_constant_identifier_names

import 'package:cogni_chat/src/biz_logic/data/dto/openai/completion/message/message.f.dart';
import 'package:freezed_annotation/freezed_annotation.dart';

part 'completion_req.f.freezed.dart';

part 'completion_req.f.g.dart';

@freezed
class CompletionReq with _$CompletionReq {
  const factory CompletionReq({
    /// gpt-3.5-turbo
    @Default('gpt-3.5-turbo') String model,
    required List<Message> messages,

    /// What sampling temperature to use, between 0 and 2.
    /// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    /// We generally recommend altering this or top_p but not both.
    @Default(1) double temperature,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    @Default(1) double top_p,
    @Default(1) int n,
    @Default(2048) int max_tokens,

    /// Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    @Default(1) double presence_penalty,

    /// Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    @Default(1) double frequency_penalty,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
    /// Mathematically, the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
    /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    String? logit_bias,
    String? user,
  }) = _CompletionReq;

  factory CompletionReq.fromJson(Map<String, dynamic> json) => _$CompletionReqFromJson(json);
}

extension CompletionReqExt on CompletionReq {
  static double temperatureFromStyle(int style) {
    switch (style) {
      case 0:
        return 1.5;
      case 2:
        return 0.5;
      default:
        return 1;
    }
  }
}
