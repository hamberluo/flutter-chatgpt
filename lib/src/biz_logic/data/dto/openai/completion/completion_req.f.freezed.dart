// coverage:ignore-file
// GENERATED CODE - DO NOT MODIFY BY HAND
// ignore_for_file: type=lint
// ignore_for_file: unused_element, deprecated_member_use, deprecated_member_use_from_same_package, use_function_type_syntax_for_parameters, unnecessary_const, avoid_init_to_null, invalid_override_different_default_values_named, prefer_expression_function_bodies, annotate_overrides, invalid_annotation_target, unnecessary_question_mark

part of 'completion_req.f.dart';

// **************************************************************************
// FreezedGenerator
// **************************************************************************

T _$identity<T>(T value) => value;

final _privateConstructorUsedError = UnsupportedError(
    'It seems like you constructed your class using `MyClass._()`. This constructor is only meant to be used by freezed and you are not supposed to need it nor use it.\nPlease check the documentation here for more information: https://github.com/rrousselGit/freezed#custom-getters-and-methods');

CompletionReq _$CompletionReqFromJson(Map<String, dynamic> json) {
  return _CompletionReq.fromJson(json);
}

/// @nodoc
mixin _$CompletionReq {
  /// gpt-3.5-turbo
  String get model => throw _privateConstructorUsedError;
  List<Message> get messages => throw _privateConstructorUsedError;

  /// What sampling temperature to use, between 0 and 2.
  /// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  /// We generally recommend altering this or top_p but not both.
  double get temperature => throw _privateConstructorUsedError;

  /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
  /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  /// We generally recommend altering this or temperature but not both.
  double get top_p => throw _privateConstructorUsedError;
  int get n => throw _privateConstructorUsedError;
  int get max_tokens => throw _privateConstructorUsedError;

  /// Number between -2.0 and 2.0.
  /// Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
  double get presence_penalty => throw _privateConstructorUsedError;

  /// Number between -2.0 and 2.0.
  /// Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
  double get frequency_penalty => throw _privateConstructorUsedError;

  /// Modify the likelihood of specified tokens appearing in the completion.
  /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
  /// Mathematically, the bias is added to the logits generated by the model prior to sampling.
  /// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
  /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
  String? get logit_bias => throw _privateConstructorUsedError;
  String? get user => throw _privateConstructorUsedError;

  Map<String, dynamic> toJson() => throw _privateConstructorUsedError;
  @JsonKey(ignore: true)
  $CompletionReqCopyWith<CompletionReq> get copyWith =>
      throw _privateConstructorUsedError;
}

/// @nodoc
abstract class $CompletionReqCopyWith<$Res> {
  factory $CompletionReqCopyWith(
          CompletionReq value, $Res Function(CompletionReq) then) =
      _$CompletionReqCopyWithImpl<$Res, CompletionReq>;
  @useResult
  $Res call(
      {String model,
      List<Message> messages,
      double temperature,
      double top_p,
      int n,
      int max_tokens,
      double presence_penalty,
      double frequency_penalty,
      String? logit_bias,
      String? user});
}

/// @nodoc
class _$CompletionReqCopyWithImpl<$Res, $Val extends CompletionReq>
    implements $CompletionReqCopyWith<$Res> {
  _$CompletionReqCopyWithImpl(this._value, this._then);

  // ignore: unused_field
  final $Val _value;
  // ignore: unused_field
  final $Res Function($Val) _then;

  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? model = null,
    Object? messages = null,
    Object? temperature = null,
    Object? top_p = null,
    Object? n = null,
    Object? max_tokens = null,
    Object? presence_penalty = null,
    Object? frequency_penalty = null,
    Object? logit_bias = freezed,
    Object? user = freezed,
  }) {
    return _then(_value.copyWith(
      model: null == model
          ? _value.model
          : model // ignore: cast_nullable_to_non_nullable
              as String,
      messages: null == messages
          ? _value.messages
          : messages // ignore: cast_nullable_to_non_nullable
              as List<Message>,
      temperature: null == temperature
          ? _value.temperature
          : temperature // ignore: cast_nullable_to_non_nullable
              as double,
      top_p: null == top_p
          ? _value.top_p
          : top_p // ignore: cast_nullable_to_non_nullable
              as double,
      n: null == n
          ? _value.n
          : n // ignore: cast_nullable_to_non_nullable
              as int,
      max_tokens: null == max_tokens
          ? _value.max_tokens
          : max_tokens // ignore: cast_nullable_to_non_nullable
              as int,
      presence_penalty: null == presence_penalty
          ? _value.presence_penalty
          : presence_penalty // ignore: cast_nullable_to_non_nullable
              as double,
      frequency_penalty: null == frequency_penalty
          ? _value.frequency_penalty
          : frequency_penalty // ignore: cast_nullable_to_non_nullable
              as double,
      logit_bias: freezed == logit_bias
          ? _value.logit_bias
          : logit_bias // ignore: cast_nullable_to_non_nullable
              as String?,
      user: freezed == user
          ? _value.user
          : user // ignore: cast_nullable_to_non_nullable
              as String?,
    ) as $Val);
  }
}

/// @nodoc
abstract class _$$_CompletionReqCopyWith<$Res>
    implements $CompletionReqCopyWith<$Res> {
  factory _$$_CompletionReqCopyWith(
          _$_CompletionReq value, $Res Function(_$_CompletionReq) then) =
      __$$_CompletionReqCopyWithImpl<$Res>;
  @override
  @useResult
  $Res call(
      {String model,
      List<Message> messages,
      double temperature,
      double top_p,
      int n,
      int max_tokens,
      double presence_penalty,
      double frequency_penalty,
      String? logit_bias,
      String? user});
}

/// @nodoc
class __$$_CompletionReqCopyWithImpl<$Res>
    extends _$CompletionReqCopyWithImpl<$Res, _$_CompletionReq>
    implements _$$_CompletionReqCopyWith<$Res> {
  __$$_CompletionReqCopyWithImpl(
      _$_CompletionReq _value, $Res Function(_$_CompletionReq) _then)
      : super(_value, _then);

  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? model = null,
    Object? messages = null,
    Object? temperature = null,
    Object? top_p = null,
    Object? n = null,
    Object? max_tokens = null,
    Object? presence_penalty = null,
    Object? frequency_penalty = null,
    Object? logit_bias = freezed,
    Object? user = freezed,
  }) {
    return _then(_$_CompletionReq(
      model: null == model
          ? _value.model
          : model // ignore: cast_nullable_to_non_nullable
              as String,
      messages: null == messages
          ? _value._messages
          : messages // ignore: cast_nullable_to_non_nullable
              as List<Message>,
      temperature: null == temperature
          ? _value.temperature
          : temperature // ignore: cast_nullable_to_non_nullable
              as double,
      top_p: null == top_p
          ? _value.top_p
          : top_p // ignore: cast_nullable_to_non_nullable
              as double,
      n: null == n
          ? _value.n
          : n // ignore: cast_nullable_to_non_nullable
              as int,
      max_tokens: null == max_tokens
          ? _value.max_tokens
          : max_tokens // ignore: cast_nullable_to_non_nullable
              as int,
      presence_penalty: null == presence_penalty
          ? _value.presence_penalty
          : presence_penalty // ignore: cast_nullable_to_non_nullable
              as double,
      frequency_penalty: null == frequency_penalty
          ? _value.frequency_penalty
          : frequency_penalty // ignore: cast_nullable_to_non_nullable
              as double,
      logit_bias: freezed == logit_bias
          ? _value.logit_bias
          : logit_bias // ignore: cast_nullable_to_non_nullable
              as String?,
      user: freezed == user
          ? _value.user
          : user // ignore: cast_nullable_to_non_nullable
              as String?,
    ));
  }
}

/// @nodoc
@JsonSerializable()
class _$_CompletionReq implements _CompletionReq {
  const _$_CompletionReq(
      {this.model = 'gpt-3.5-turbo',
      required final List<Message> messages,
      this.temperature = 1,
      this.top_p = 1,
      this.n = 1,
      this.max_tokens = 2048,
      this.presence_penalty = 1,
      this.frequency_penalty = 1,
      this.logit_bias,
      this.user})
      : _messages = messages;

  factory _$_CompletionReq.fromJson(Map<String, dynamic> json) =>
      _$$_CompletionReqFromJson(json);

  /// gpt-3.5-turbo
  @override
  @JsonKey()
  final String model;
  final List<Message> _messages;
  @override
  List<Message> get messages {
    if (_messages is EqualUnmodifiableListView) return _messages;
    // ignore: implicit_dynamic_type
    return EqualUnmodifiableListView(_messages);
  }

  /// What sampling temperature to use, between 0 and 2.
  /// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  /// We generally recommend altering this or top_p but not both.
  @override
  @JsonKey()
  final double temperature;

  /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
  /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  /// We generally recommend altering this or temperature but not both.
  @override
  @JsonKey()
  final double top_p;
  @override
  @JsonKey()
  final int n;
  @override
  @JsonKey()
  final int max_tokens;

  /// Number between -2.0 and 2.0.
  /// Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
  @override
  @JsonKey()
  final double presence_penalty;

  /// Number between -2.0 and 2.0.
  /// Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
  @override
  @JsonKey()
  final double frequency_penalty;

  /// Modify the likelihood of specified tokens appearing in the completion.
  /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
  /// Mathematically, the bias is added to the logits generated by the model prior to sampling.
  /// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
  /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
  @override
  final String? logit_bias;
  @override
  final String? user;

  @override
  String toString() {
    return 'CompletionReq(model: $model, messages: $messages, temperature: $temperature, top_p: $top_p, n: $n, max_tokens: $max_tokens, presence_penalty: $presence_penalty, frequency_penalty: $frequency_penalty, logit_bias: $logit_bias, user: $user)';
  }

  @override
  bool operator ==(dynamic other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$_CompletionReq &&
            (identical(other.model, model) || other.model == model) &&
            const DeepCollectionEquality().equals(other._messages, _messages) &&
            (identical(other.temperature, temperature) ||
                other.temperature == temperature) &&
            (identical(other.top_p, top_p) || other.top_p == top_p) &&
            (identical(other.n, n) || other.n == n) &&
            (identical(other.max_tokens, max_tokens) ||
                other.max_tokens == max_tokens) &&
            (identical(other.presence_penalty, presence_penalty) ||
                other.presence_penalty == presence_penalty) &&
            (identical(other.frequency_penalty, frequency_penalty) ||
                other.frequency_penalty == frequency_penalty) &&
            (identical(other.logit_bias, logit_bias) ||
                other.logit_bias == logit_bias) &&
            (identical(other.user, user) || other.user == user));
  }

  @JsonKey(ignore: true)
  @override
  int get hashCode => Object.hash(
      runtimeType,
      model,
      const DeepCollectionEquality().hash(_messages),
      temperature,
      top_p,
      n,
      max_tokens,
      presence_penalty,
      frequency_penalty,
      logit_bias,
      user);

  @JsonKey(ignore: true)
  @override
  @pragma('vm:prefer-inline')
  _$$_CompletionReqCopyWith<_$_CompletionReq> get copyWith =>
      __$$_CompletionReqCopyWithImpl<_$_CompletionReq>(this, _$identity);

  @override
  Map<String, dynamic> toJson() {
    return _$$_CompletionReqToJson(
      this,
    );
  }
}

abstract class _CompletionReq implements CompletionReq {
  const factory _CompletionReq(
      {final String model,
      required final List<Message> messages,
      final double temperature,
      final double top_p,
      final int n,
      final int max_tokens,
      final double presence_penalty,
      final double frequency_penalty,
      final String? logit_bias,
      final String? user}) = _$_CompletionReq;

  factory _CompletionReq.fromJson(Map<String, dynamic> json) =
      _$_CompletionReq.fromJson;

  @override

  /// gpt-3.5-turbo
  String get model;
  @override
  List<Message> get messages;
  @override

  /// What sampling temperature to use, between 0 and 2.
  /// Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  /// We generally recommend altering this or top_p but not both.
  double get temperature;
  @override

  /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
  /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  /// We generally recommend altering this or temperature but not both.
  double get top_p;
  @override
  int get n;
  @override
  int get max_tokens;
  @override

  /// Number between -2.0 and 2.0.
  /// Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
  double get presence_penalty;
  @override

  /// Number between -2.0 and 2.0.
  /// Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
  double get frequency_penalty;
  @override

  /// Modify the likelihood of specified tokens appearing in the completion.
  /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
  /// Mathematically, the bias is added to the logits generated by the model prior to sampling.
  /// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
  /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
  String? get logit_bias;
  @override
  String? get user;
  @override
  @JsonKey(ignore: true)
  _$$_CompletionReqCopyWith<_$_CompletionReq> get copyWith =>
      throw _privateConstructorUsedError;
}
